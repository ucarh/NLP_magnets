{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This NB downloads the full text data from Elsevier API using the list of DOIs obtained from CrossRef. Note that you need to have your API credentials in a config.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load configuration\n",
    "con_file = open(\"config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "## Initialize client`\n",
    "client = ElsClient(config['apikey'])\n",
    "client.inst_token = config['insttoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(doi,outfile):\n",
    "    doi_doc = FullDoc(doi = doi)\n",
    "    \n",
    "    if doi_doc.read(client):\n",
    "        try:\n",
    "            title = doi_doc.title\n",
    "        except:\n",
    "            title=\"\"\n",
    "        try:\n",
    "            abstract = doi_doc.data['coredata']['dc:description']\n",
    "        except:\n",
    "            abstract=\"\"\n",
    "            \n",
    "        try:\n",
    "            text = doi_doc.data['originalText']\n",
    "        except:\n",
    "            text=None\n",
    "\n",
    "        #some articles start with Introduction, some start with 1 Introduction\n",
    "        if isinstance(text,str):\n",
    "            if '1 Introduction' not in text:\n",
    "                introduction=text.find('Introduction',text.find('Introduction')+1)\n",
    "            else:\n",
    "                introduction=text.find('1 Introduction',text.find('1 Introduction')+1)\n",
    "\n",
    "        \n",
    "        # Section number for the Conclusion(s) section varies, so used regex to catch those.\n",
    "        # +-750 is a heuristic number. Wanted to capture a few sentences in the conclusion section.\n",
    "            try:\n",
    "                regex=re.compile(r'\\d+\\sConclusions')\n",
    "                conclusions_occurence=regex.findall(text)[0]\n",
    "                conclusions_start=text.find(conclusions_occurence,text.find(conclusions_occurence)+1)\n",
    "                filtered_text=text[introduction:conclusions_start+750].strip()\n",
    "                # print('regex 1 found')\n",
    "            except:\n",
    "                try:\n",
    "                    regex=re.compile(r'\\d+\\sConclusion')\n",
    "                    conclusions_occurence=regex.findall(text)[0]\n",
    "                    conclusions_start=text.find(conclusions_occurence,text.find(conclusions_occurence)+1)\n",
    "                    filtered_text=text[introduction:conclusions_start+750].strip()\n",
    "                    # print('regex 2 found')\n",
    "\n",
    "        #If conclusions section is not present, use References [1] or References to get the end of the article\n",
    "                except:\n",
    "                    if 'References [1]' not in text:\n",
    "                        reference_start = text.find('References',text.find('References')+1)\n",
    "                    else:\n",
    "                        reference_start = text.find(\"References [1]\")\n",
    "                    filtered_text = text[introduction:reference_start-1000].strip()\n",
    "                    # print('regex NOT found')\n",
    "            \n",
    "            json.dump({'title': title, 'abstract': abstract, 'text': filtered_text,'doi':doi}, outfile)\n",
    "        else:\n",
    "            filtered_text=\"Text is not string.\"\n",
    "            json.dump({'title': title, 'abstract': abstract, 'text': filtered_text,'doi':doi}, outfile)\n",
    "          \n",
    "    else:\n",
    "        filtered_text=\"Read document failed.\"\n",
    "        json.dump({'title': title, 'abstract': abstract, 'text': filtered_text,'doi':doi}, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are reading the list of DOIs and the code below downloads the full text articles and loads in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023=pd.read_csv('./DOI_Elsevier_magnetic/2023_Elsevier_data.csv')\n",
    "DOI_list=df_2023['DOI'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a \"./data/2023_magnetic_corpus.jsonl\" file.\n",
    "count=1\n",
    "with open('data/2023_magnetic_corpus.jsonl', 'w') as outfile:\n",
    "    for doi in DOI_list:\n",
    "        with open(\"data/2023_magnetic_corpus_progress.txt\",\"a\") as file:\n",
    "            file.write(f\"working on doi:{doi}, step {count}/{len(DOI_list)} \\n\")\n",
    "        count+=1\n",
    "        \n",
    "        create_dataset(doi, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
