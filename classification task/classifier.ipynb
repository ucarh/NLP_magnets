{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains classifier model to filter out the papers that are relevant and not very relevant for magnetic materials research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to set the random seed\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to concatenate two given path components, and create this directory if it doesn't exists\n",
    "def join_and_create_folder(main_f, sub_f, sub_f_replace_hifen = False, ):\n",
    "    if sub_f_replace_hifen == True:\n",
    "        sub_f = sub_f.replace(\"-\", \"_\")\n",
    "        sub_f = sub_f.replace(\"/\", \"_\")\n",
    "    MODEL_DIR=os.path.join(main_f, sub_f)\n",
    "    isExist = os.path.exists(MODEL_DIR)\n",
    "    if not isExist:\n",
    "        os.mkdir(MODEL_DIR)\n",
    "    return MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"title_abstract\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to get tain-, validation- and test- loaders from the dataset\n",
    "def get_data_loaders(data_root_):\n",
    "    magnetics_dataset = load_from_disk(data_root_)\n",
    "    # magnetics_dataset\n",
    "    ### Remove the columns corresponding to values the model does not expect.\n",
    "    magnetics_dataset = magnetics_dataset.remove_columns(['title', 'abstract', 'text', 'doi', 'abstract_length'])\n",
    "    ### Tokenize and remove the columns corresponding to values the model does not expect.\n",
    "    magnetics_dataset_tokenized = magnetics_dataset.map(tokenize_function, batched=True, remove_columns=[\"title_abstract\"])\n",
    "\n",
    "    df_pandas_train = pd.DataFrame(magnetics_dataset_tokenized['train'])\n",
    "    df_pandas_train = df_pandas_train[['input_ids', 'token_type_ids', 'attention_mask', 'labels']]\n",
    "\n",
    "    df_pandas_val = pd.DataFrame(magnetics_dataset_tokenized['val'])\n",
    "    df_pandas_val = df_pandas_val[['input_ids', 'token_type_ids', 'attention_mask', 'labels']]\n",
    "\n",
    "    df_pandas_test = pd.DataFrame(magnetics_dataset_tokenized['test'])\n",
    "    df_pandas_test = df_pandas_test[['input_ids', 'token_type_ids', 'attention_mask', 'labels']]\n",
    "\n",
    "    magnetics_dataset_tokenized['train'] = Dataset.from_pandas(df_pandas_train)\n",
    "    magnetics_dataset_tokenized['val'] = Dataset.from_pandas(df_pandas_val)\n",
    "    magnetics_dataset_tokenized['test'] = Dataset.from_pandas(df_pandas_test)\n",
    "\n",
    "    ### Set the format of the datasets so they return PyTorch tensors instead of lists\n",
    "    magnetics_dataset_tokenized.set_format(\"torch\")\n",
    "    # magnetics_dataset_tokenized\n",
    "    # magnetics_dataset_tokenized['train']['input_ids'][3].shape  # token_type_ids, attention_mask, labels\n",
    "\n",
    "    set_seed(0)\n",
    "    train_dataloader = DataLoader(magnetics_dataset_tokenized['train'], shuffle=True, batch_size=train_batch_size)\n",
    "    val_dataloader = DataLoader(magnetics_dataset_tokenized['val'], shuffle=True, batch_size=val_batch_size)\n",
    "    test_dataloader = DataLoader(magnetics_dataset_tokenized['test'], batch_size=test_batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "# train_dataloader, val_dataloader, test_dataloader = get_data_loaders(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation funtion used for the validation\n",
    "def eval_loss_acc_entr(loader, model, device):\n",
    "    entropLST = []; AccLST = []; lossLST = []\n",
    "    with torch.inference_mode():\n",
    "#     with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss_val = outputs.loss\n",
    "            lossLST.append(loss_val.item())\n",
    "            \n",
    "            y_hat = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            entr = torch.special.entr(y_hat)\n",
    "            entrop = torch.mean(torch.sum(entr,axis=1)).item()\n",
    "            entropLST.append(entrop)\n",
    "            \n",
    "            Acc = torch.mean( (batch['labels'] == torch.argmax(y_hat,axis=1)).float()).item()\n",
    "            AccLST.append(Acc)\n",
    "        \n",
    "    return np.mean(AccLST), np.mean(entropLST), np.mean(lossLST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the parameters like batch size and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size = 16\n",
    "val_batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "data_root = \"../Corpus/magnetics_train_val_test_by_text\"\n",
    "\n",
    "device = torch.device(\"cuda:7\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the weights of the four pretrained models and adding a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a369fdbe5448428ca6554bab6b853553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c099187d184152b448301c0f5339fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beb75542cb846ca82a001e4fc047ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca4c4b5dec4f90954c63a3ff462b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint=\"bert-base-uncased\"\n",
    "# model_checkpoint=\"m3rg-iitd/matscibert\"\n",
    "# model_checkpoint=\"nlp-magnets/magbert\"        #bert-base trained on magnet corpus\n",
    "# model_checkpoint=\"nlp-magnets/magmatbert\"     #matscibert trained on magnet corpus\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to train the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_train(num_epochs, learning_rate, train_dataloader, val_dataloader, model_checkpoint, train_batch_size):\n",
    "    set_seed(0)\n",
    "\n",
    "    train_loader_len = len(train_dataloader)\n",
    "    num_training_steps = num_epochs * train_loader_len\n",
    "\n",
    "    LOG_DIR='./outputs'\n",
    "    MODEL_DIR = join_and_create_folder(LOG_DIR, model_checkpoint, True)\n",
    "    \n",
    "    lgfile = \"numEpoch_%.0f_trainBS_%.0f_lr_%.8f\"%(num_epochs, train_batch_size, learning_rate)\n",
    "    lgfile_DIR = join_and_create_folder(MODEL_DIR, lgfile, False)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW( model.parameters(), lr=learning_rate) \n",
    "    lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    progress_bar=tqdm(range(num_training_steps))\n",
    "    \n",
    "    ep = 0; max_accuracy_val= 0.;\n",
    "    losses_train = []; losses_train_epoch = []\n",
    "    accuracy_train = []; accuracy_train_epoch = []\n",
    "    losses_val = []; losses_val_epoch = []\n",
    "    accuracy_val = []; accuracy_val_epoch = []\n",
    "    entropy_train_epoch = []; entropy_val_epoch =[]\n",
    "    losses_val_epoch_01 = []; accuracy_val_epoch_01 = []; entropy_val_epoch_01 =[]\n",
    "\n",
    "    acc_train, ent_train, loss_train = eval_loss_acc_entr(train_dataloader, model, device)\n",
    "    accuracy_train_epoch.append(acc_train); entropy_train_epoch.append(ent_train); losses_train_epoch.append(loss_train)\n",
    "\n",
    "    acc_val, ent_val, loss_val = eval_loss_acc_entr(val_dataloader, model, device)\n",
    "    accuracy_val_epoch.append(acc_val); entropy_val_epoch.append(ent_val); losses_val_epoch.append(loss_val)\n",
    "    accuracy_val_epoch_01.append([ep,acc_val]); entropy_val_epoch_01.append([ep,ent_val]); losses_val_epoch_01.append([ep,loss_val]) \n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            y_hat = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            losses_train.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy_train.append( torch.mean((batch[\"labels\"] == torch.argmax(y_hat,axis=1)).float()).item() )\n",
    "            entropy_train = torch.special.entr(y_hat)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            lr_scheduler.step() \n",
    "\n",
    "            ### evaluating after every 0.1 epochs\n",
    "            model.eval()\n",
    "            if batch_idx % (np.floor(train_loader_len/10)) == 0 and batch_idx != 0:\n",
    "                ep += 0.1\n",
    "\n",
    "                accuracy_val_01, entropy_val_01, losses_val_01 = eval_loss_acc_entr(val_dataloader, model, device)\n",
    "                accuracy_val_epoch_01.append([ep,accuracy_val_01]); entropy_val_epoch_01.append([ep,entropy_val_01]) \n",
    "                losses_val_epoch_01.append([ep,losses_val_01]) \n",
    "                \n",
    "                if accuracy_val_01 > max_accuracy_val:\n",
    "                    old_max_acc = max_accuracy_val\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                    checkpoint = {\n",
    "                        'epoch': np.round(ep, 3),\n",
    "                        'model_state': model.state_dict(),\n",
    "                        'optimizer_state': optimizer.state_dict(),\n",
    "                        'lr_sched': lr_scheduler.state_dict(),\n",
    "                    }\n",
    "                   \n",
    "                    best_filename = os.path.join(lgfile_DIR + \"/best_checkpoint.pth\")\n",
    "                    torch.save(checkpoint, best_filename)\n",
    "                    model.save_pretrained(lgfile_DIR)\n",
    "                    tokenizer.save_pretrained(lgfile_DIR)\n",
    "\n",
    "                    max_accuracy_val = accuracy_val_01\n",
    "                   \n",
    "        losses_train_epoch.append(np.mean(losses_train))\n",
    "        accuracy_train_epoch.append(np.mean(accuracy_train))\n",
    "        entropy_train_epoch.append(torch.mean(torch.sum(entropy_train,axis=1)).item())\n",
    "\n",
    "\n",
    "        ### evaluating after every epoch\n",
    "        accuracy_val, entropy_val, losses_val = eval_loss_acc_entr(val_dataloader, model, device)\n",
    "        losses_val_epoch.append(losses_val); accuracy_val_epoch.append(accuracy_val); entropy_val_epoch.append(entropy_val)\n",
    "\n",
    "        if accuracy_val > max_accuracy_val:\n",
    "            old_max_acc = max_accuracy_val\n",
    "            best_epoch = epoch\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'lr_sched': lr_scheduler.state_dict(),\n",
    "            }\n",
    "            \n",
    "            best_filename = os.path.join(lgfile_DIR + \"/best_checkpoint.pth\")\n",
    "            torch.save(checkpoint, best_filename)\n",
    "            model.save_pretrained(lgfile_DIR)\n",
    "            tokenizer.save_pretrained(lgfile_DIR)\n",
    "\n",
    "            max_accuracy_val = accuracy_val\n",
    "\n",
    "    dict = {\"train_accuracy\": accuracy_train_epoch, \"train_loss\": losses_train_epoch,\\\n",
    "            \"train_entr\": entropy_train_epoch, \"val_accuracy\": accuracy_val_epoch, \\\n",
    "            \"val_loss\": losses_val_epoch, \"val_entr\": entropy_val_epoch, \\\n",
    "            \"val_accuracy_01\": accuracy_val_epoch_01, \"val_loss_01\": losses_val_epoch_01,\\\n",
    "            \"val_entr_01\": entropy_val_epoch_01, \"lr\": learning_rate, \"train_BS\": train_batch_size, \"num_of_epochs\": num_epochs}\n",
    "\n",
    "    logfile = lgfile_DIR +\"/numEpoch_%.0f_trainBS_%.0f_lr_%.6f_max_%.8f.log\"%(num_epochs, train_batch_size, learning_rate, max_accuracy_val)\n",
    "    pkl.dump(dict, open(logfile,\"wb\"))\n",
    "    \n",
    "    print(\"saved at:\", lgfile_DIR)\n",
    "    \n",
    "    return max_accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c51e7ea48d947a793ce53bda10efb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6af1996b3a4c6c9b19d2625971393d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "lr_lst = [1e-5, 2e-5, 5e-5]\n",
    "# batch_size_lst = [32]  #16\n",
    "\n",
    "# for num_epochs, learning_rate, train_batch_size in itertools.product(epoch_lst, lr_lst, batch_size_lst):\n",
    "for learning_rate in lr_lst:\n",
    "    train_dataloader, val_dataloader, test_dataloader = get_data_loaders(data_root)\n",
    "    classification_train(num_epochs, learning_rate, train_dataloader, val_dataloader, model_checkpoint, train_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magnets",
   "language": "python",
   "name": "magnets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
