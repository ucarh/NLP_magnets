{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import collections\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code below is a sampler that ensures that dataloader continues from where it is leff off after the training breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/60993677/how-can-i-save-pytorchs-dataloader-instance\n",
    "import random\n",
    "from torch.utils.data.dataloader import Sampler\n",
    "\n",
    "random.seed(0)  # use a fixed number\n",
    "\n",
    "\n",
    "class MySampler(Sampler):\n",
    "    def __init__(self, n, seed=0):\n",
    "        self.n = n\n",
    "        np.random.seed = seed  \n",
    "        random.seed(seed)  # use a fixed number\n",
    "\n",
    "        self.seq = list(range(n))\n",
    "        np.random.shuffle(self.seq)\n",
    "\n",
    "    def reset(self, seed):\n",
    "        np.random.seed = seed\n",
    "        random.seed(seed)  \n",
    "        self.seq = list(range(self.n))\n",
    "        np.random.shuffle(self.seq)\n",
    "\n",
    "\n",
    "    def shrink(self,i):\n",
    "        self.seq = self.seq[i:]\n",
    "\n",
    "    def __iter__(self):         \n",
    "        return iter(self.seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_magnetics_ds=load_from_disk(\"/l/users/huseyin.ucar/NLP_magnets_data/chunked_ds_matscibert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "         num_rows: 972026\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "         num_rows: 106523\n",
       "     })\n",
       " }),\n",
       " 34027.875)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_magnetics_ds, 1088892/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/l/users/huseyin.ucar/NLP_magnets_data/checkpoint/matscibert_model_linear_sched_50_epochs_32_BS_last.pth'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cfg:\n",
    "    batch_size=32\n",
    "    num_train_epochs = 50\n",
    "    wwm_probability = 0.15\n",
    "    DEBUG=False  \n",
    "    \n",
    "    scheduler='linear'\n",
    "    model_checkpoint=\"m3rg-iitd/matscibert\"\n",
    "    KERNEL_TYPE=\"matscibert\"+'_model_'+scheduler+'_sched_'+str(num_train_epochs)+'_epochs_'+str(batch_size)+'_BS'\n",
    "    LOG_DIR='./logs'\n",
    "    DATA_DIR=\"/l/users/huseyin.ucar/NLP_magnets_data\"\n",
    "    MODEL_DIR=os.path.join(DATA_DIR,\"weights\")\n",
    "    CHECKPOINT_DIR=os.path.join(DATA_DIR,\"checkpoint\")\n",
    "\n",
    "    load_model = False\n",
    "    epoch_cont=0\n",
    "    load_path = model_file=os.path.join(MODEL_DIR, f'{KERNEL_TYPE}_best.pth')\n",
    "    \n",
    "checkpoint_file=os.path.join(cfg.CHECKPOINT_DIR, f'{cfg.KERNEL_TYPE}_last.pth')\n",
    "checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'matscibert_model_linear_sched_50_epochs_32_BS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(cfg.LOG_DIR):\n",
    "    os.makedirs(cfg.LOG_DIR)\n",
    "\n",
    "if not os.path.exists(cfg.MODEL_DIR):\n",
    "    os.makedirs(cfg.MODEL_DIR)\n",
    "\n",
    "if not os.path.exists(cfg.CHECKPOINT_DIR):\n",
    "    os.makedirs(cfg.CHECKPOINT_DIR)\n",
    "\n",
    "\n",
    "cfg.KERNEL_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(cfg.model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whole word masking function is implemented below rather than masking the tokens only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        words_ids=feature.pop(\"word_ids\")\n",
    "\n",
    "        mapping=collections.defaultdict(list)\n",
    "        current_word_index=-1\n",
    "        current_word=None\n",
    "\n",
    "        for idx,word_id in enumerate(words_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id !=current_word:\n",
    "                    current_word=word_id\n",
    "                    current_word_index+=1\n",
    "                mapping[current_word_index].append(idx)\n",
    "        \n",
    "        mask=np.random.binomial(1,cfg.wwm_probability,(len(mapping),))\n",
    "\n",
    "        input_ids=feature['input_ids']\n",
    "        labels=feature['labels']\n",
    "        new_labels=[-100]*len(labels)\n",
    "\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id=word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx]=labels[idx]\n",
    "                input_ids[idx]=tokenizer.mask_token_id\n",
    "        feature[\"labels\"]=new_labels\n",
    "\n",
    "    return default_data_collator(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = whole_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.DEBUG:\n",
    "    train_size = 500\n",
    "    test_size = int(0.1 * train_size)\n",
    "\n",
    "    chunked_magnetics_ds = chunked_magnetics_ds[\"train\"].train_test_split(train_size=train_size, test_size=test_size, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 972026\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 106523\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_magnetics_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89029d11cfa49dd80769e89a68d1322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/106523 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset=chunked_magnetics_ds['test'].map(insert_random_mask,batched=True,num_proc=8,\\\n",
    "                                              remove_columns=chunked_magnetics_ds['test'].column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "        \"masked_token_type_ids\": \"token_type_ids\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainSampler = MySampler(chunked_magnetics_ds['train'].num_rows)\n",
    "\n",
    "train_dataloader=DataLoader(chunked_magnetics_ds['train'],\n",
    "                            shuffle=False,\n",
    "                            batch_size=cfg.batch_size,\n",
    "                            sampler = trainSampler,\n",
    "                            collate_fn=whole_word_masking_data_collator)\n",
    "\n",
    "eval_dataloader=DataLoader(eval_dataset,\n",
    "                           batch_size=cfg.batch_size,\n",
    "                           collate_fn=default_data_collator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "import os \n",
    "import torch\n",
    "from huggingface_hub import create_repo,get_full_repo_name,Repository\n",
    "from tqdm.auto import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=31090, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=AutoModelForMaskedLM.from_pretrained(cfg.model_checkpoint)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### below we are loading the checkpoints and model parameters to ensure the training continutes from the state it was before it breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = None\n",
    "seq_sampler_i = None\n",
    "best_score=float(\"inf\")\n",
    "currentEpoch = 0\n",
    "\n",
    "if  os.path.exists(checkpoint_file):\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "\n",
    "     \n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_sched'])\n",
    "\n",
    "\n",
    "    best_score = checkpoint['best_score']\n",
    "    seq =  checkpoint['seq']\n",
    "    seq_sampler_i = checkpoint['seq_sampler_i']\n",
    "    currentEpoch = checkpoint['currentEpoch'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/nlp-magnets/matscibert_model_linear_sched_50_epochs_32_BS into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "#uncomment below only if the repo is not present on huggingface.\n",
    "\n",
    "# create_repo(f\"nlp-magnets/{cfg.KERNEL_TYPE}\",private=True,repo_type=\"model\")\n",
    "\n",
    "repo_name=get_full_repo_name(model_id=cfg.KERNEL_TYPE,organization=\"nlp-magnets\")\n",
    "repo_name\n",
    "output_dir=os.path.join(cfg.DATA_DIR,cfg.KERNEL_TYPE)\n",
    "repo = Repository(output_dir, clone_from=repo_name)\n",
    "repo.git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(samplesProcessed, currentEpoch):\n",
    "    print(\"saving checkpoint\")\n",
    "    checkpoint = { \n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_sched': lr_scheduler.state_dict(),\n",
    "                 'best_score':  best_score ,\n",
    "                 'seq': trainSampler.seq,\n",
    "                  'seq_sampler_i' : samplesProcessed,\n",
    "                  'currentEpoch' : currentEpoch            \n",
    "                }\n",
    "    torch.save(checkpoint, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop for training which is completed in 50 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff225138b9b14eb1a7440ea4fd3f026b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40030b1b78e4d8387fd54c1f16b7aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation.....\n",
      "Fri Nov 10 23:06:51 2023 Epoch 0, Perplexity: 30.22263873225493\n",
      "score (inf --> 30.22264). Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e20b698fa9f4c7eac8936ebb8ae11e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/nlp-magnets/bert-base-uncased_model_linear_sched_3_epochs_32_BS\n",
      "   84a355a..c5ff293  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final checkpoint\n",
      "saving checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e968875c9ad445ca8cf245b8b0e5960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/huseyin.ucar/NLP_magnets/pretraining/MLM_BERT_uncased_10hrs_chkpt.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscc/home/huseyin.ucar/NLP_magnets/pretraining/MLM_BERT_uncased_10hrs_chkpt.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m=\u001b[39moutputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscc/home/huseyin.ucar/NLP_magnets/pretraining/MLM_BERT_uncased_10hrs_chkpt.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcscc/home/huseyin.ucar/NLP_magnets/pretraining/MLM_BERT_uncased_10hrs_chkpt.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscc/home/huseyin.ucar/NLP_magnets/pretraining/MLM_BERT_uncased_10hrs_chkpt.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcscc/home/huseyin.ucar/NLP_magnets/pretraining/MLM_BERT_uncased_10hrs_chkpt.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-38/lib/python3.8/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-38/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-38/lib/python3.8/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     adamw(\n\u001b[1;32m    185\u001b[0m         params_with_grad,\n\u001b[1;32m    186\u001b[0m         grads,\n\u001b[1;32m    187\u001b[0m         exp_avgs,\n\u001b[1;32m    188\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    189\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    190\u001b[0m         state_steps,\n\u001b[1;32m    191\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    192\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    193\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    194\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    195\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    196\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    197\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    198\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    199\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    200\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    201\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    202\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    203\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-38/lib/python3.8/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m func(\n\u001b[1;32m    336\u001b[0m     params,\n\u001b[1;32m    337\u001b[0m     grads,\n\u001b[1;32m    338\u001b[0m     exp_avgs,\n\u001b[1;32m    339\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    340\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    341\u001b[0m     state_steps,\n\u001b[1;32m    342\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    343\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    344\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    345\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    346\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    347\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    348\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    349\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    350\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    351\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    352\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    353\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-38/lib/python3.8/site-packages/torch/optim/adamw.py:464\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    462\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    463\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    466\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    468\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "progress_bar=tqdm(range(num_training_steps))\n",
    "\n",
    "if currentEpoch < cfg.num_train_epochs:\n",
    "    doComputation = True\n",
    "else:\n",
    "    doComputation = False\n",
    "\n",
    "BRAKED_TRAINING = False\n",
    "\n",
    "while doComputation:\n",
    "\n",
    "    model.train()\n",
    "    i = 0\n",
    "\n",
    "    if seq is not None:\n",
    "        print(\"reseting SEQ train sampler\")\n",
    "        trainSampler.seq = seq\n",
    "        trainSampler.shrink(seq_sampler_i)\n",
    "\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        i+= cfg.batch_size\n",
    "        batch={k:v.to(device) for k,v in batch.items()}\n",
    "        outputs=model(**batch)\n",
    "        loss=outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        if time.time() - start_time > 10*60*60:\n",
    "            # make checkpoint and break\n",
    "            print(\"going to break\")\n",
    "            saveCheckpoint(i, currentEpoch )\n",
    "            doComputation = False\n",
    "            BRAKED_TRAINING = True\n",
    "            print(\"BREAK TRAINING\")\n",
    "            break\n",
    "\n",
    "    if not BRAKED_TRAINING:\n",
    "        print(\"evaluation.....\")\n",
    "        model.eval()\n",
    "        losses=[]\n",
    "        \n",
    "        for step,batch in enumerate(eval_dataloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs=model(**batch)\n",
    "            loss=outputs.loss\n",
    "            losses.append(loss.repeat(cfg.batch_size))\n",
    "        \n",
    "        losses=torch.cat(losses)\n",
    "        losses=losses[:len(eval_dataset)]\n",
    "\n",
    "        try:\n",
    "            perplexity=math.exp(torch.mean(losses))\n",
    "        except OverflowError:\n",
    "            perplexity=float(\"inf\")\n",
    "\n",
    "        \n",
    "        content=time.ctime()+' '+f'Epoch {currentEpoch}, Perplexity: {perplexity}'\n",
    "        print(content)\n",
    "\n",
    "        with open(os.path.join(cfg.LOG_DIR, f'log_{cfg.KERNEL_TYPE}.txt'),'a')\\\n",
    "            as appender:\n",
    "            appender.write(content + '\\n')\n",
    "\n",
    "        model_file=os.path.join(cfg.MODEL_DIR, f'{cfg.KERNEL_TYPE}_best.pth')\n",
    "        if best_score > perplexity:\n",
    "            print('score ({:.5f} --> {:.5f}). Saving model ...'.format(best_score, perplexity))\n",
    "            best_score = perplexity\n",
    "\n",
    "            checkpoint = { \n",
    "                'epoch': currentEpoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_sched': lr_scheduler.state_dict()}\n",
    "            torch.save(checkpoint, model_file)\n",
    "\n",
    "            model.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            repo.push_to_hub(\"best model commit\")\n",
    "                \n",
    " \n",
    "        currentEpoch = currentEpoch+1\n",
    "        trainSampler.reset(currentEpoch)\n",
    "        seq = None \n",
    "        i = 0\n",
    "        print(\"final checkpoint\")\n",
    "        saveCheckpoint(i, currentEpoch )\n",
    "\n",
    "        if currentEpoch >= cfg.num_train_epochs :\n",
    "            model_file=os.path.join(cfg.MODEL_DIR, f'{cfg.KERNEL_TYPE}_last.pth')\n",
    "            checkpoint = { \n",
    "                'epoch': currentEpoch-1,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_sched': lr_scheduler.state_dict()}\n",
    "            torch.save(checkpoint, model_file)\n",
    "            \n",
    "            model.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            repo.push_to_hub(\"final model commit\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
